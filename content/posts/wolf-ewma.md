---
title: "An outlier-robust EWMA"
date: 2024-07-13
description: "WoLF for robust estimation of an exponentially weighted moving average."
katex: true
draft: true
tags: [kalman-filter, wolf, ewma]
tldr: "An outlier robust exponentially weighted moving average via the WoLF method."
---

Our weighted-observation likelihood filter (WolF) got accepted at ICML 2024.
Our method, which is provably-robust and easy-to-implement, *robustifies* the Kalman filter (KF)
by replacing the classical Gaussian likelihood assumption with a loss function.
Despite its simplicity, the terminology behind WoLF (similar to the terminology of the KF), might not be familiar to everyone
and the method might seem a bit abstract for newcomers.

Thus, to show the practical utility of our method,
we will show how to create a robust variant of the KF that is familiar to most: the exponentially weighted moving average (EWMA).

This post is organlised as follows:
first, we recap the EWMA.
Then, we introduce a one-dimensional state-space model (SSM) and show that the EWMA is a special case of the KF in this SSM setting.
Next, we derive the WoLF method for an EWMA.
We conclude this post by showing a numerical experiment that illustrates the robustness of the WoLF method in one-dimensional financial data.

# The exponentially weighted moving average (EWMA)
Given a sequence of observations (or measurements) $y_{1:t} = (y_1, \ldots, y_t)$,
the EWMA of the observations at time $t$ is given by
{{< math >}}
$$
m_t = \beta\,y_t + (1-\beta)\,m_{t-1},
\tag{1}
$$
{{< /math >}}
where $\beta \in (0,1]$ is the smoothing factor.
Higher levels of $\beta$ give more weight to recent observations.

# The Kalman filter in one dimension
Consider the following one-dimensional state-space model (SSM):

{{< math >}}
$$
\begin{aligned}
z_t &= z_{t-1} + w_t,\\
y_t &= z_t + e_t,
\end{aligned}
\tag{2}
$$
{{< /math >}}
where $z_t$ is the state, $w_t$ is the process noise, and $v_t$ is the measurement noise.
We assume that ${\rm var}(w_t) = q_t$ and ${\rm var}(e_t) = r_t$.[^1]
Put simply, the SSM model $(2)$ assumes that the observations $y_t$ are generated by a signal $z_t$ plus noise $e_t$.
Our goal with the Kalman filter is to estimate the signal $z_t$ given the observations $y_{1:t}$.

Our estimate of the signal $z_t$ given the observations $y_{1:t}$ is given by
{{< math >}}
$$
m_t = \mathbb{E}[z_t \vert y_{1:t}] = \int z_t\,p(z_t \vert y_{1:t}) dz_t,
\tag{4}
{{< /math >}}
$$
where $\mathbb{E}[\cdot]$ denotes the expectation operator.

The main quantity of interest in the Kalman filter in this perspective is the posterior distribution $p(z_t \vert y_{1:t})$,
which we can compute recursively using Bayes' rule.
Assuming the initial density $p(z_0) = {\cal N}(z_0 \vert m_0, s_0)$,
with ${\cal N}(\cdot \vert \mu, \sigma^2)$ a Gaussian density with mean $\mu$ and variance $\sigma^2$.
the posterior density $p(z_t \vert y_{1:t})$ is given by
{{< math >}}
$$
p(z_t \vert y_{1:t}) \propto p(y_t \vert z_t)\,p(z_t \vert y_{1:t-1}) = {\cal N}(z_t \vert m_t, s_t),
\tag{5}
$$
{{< /math >}}
with
{{< math >}}
$$
\begin{aligned}
k_t &= \frac{s_{t-1} + q_t}{s_{t-1} + q_t + r_t},\\
s_t &= k_t\,r_t,\\
m_t &= k_t\,y_t + (1-k_t)\,m_{t-1}.
\end{aligned}
\tag{6}
$$
{{< /math >}}
We defer the derivation of $(6)$ to the appendix.
From $(6)$, we see that the KF with SSM $(2)$ is equivalent to the EWMA with $\beta$ replaced by $k_t$,
i.e., **the KF is an EWMA with a time-varying smoothing factor**.
The KF formulation of the EWMA from a KF point of view
(i) provides a principled way to update the smoothing factor $k_t$,
(ii) and provides a way to estimate the uncertainty of the signal $z_t$, and
(iii) it allows us to derive the WoLF variant for the EWMA.
We do this in the next section.


# The WoLF method for the EWMA
To create a 1D version of WoLF, we
consider the SSM $(2)$ with $q_t = q$ and $r_t^2 = r^2 / w_t^2$. Here $q \geq 0$ and $r > 0$ are fixed hyperparameters.
With these assumptions, the rate $k_t$ in $(6)$ simplifies to
{{< math >}}
$$
k_t = \frac{s_{t-1} + q}{s_{t-1} + q + r^2 / w_t^2}.
\tag{7}
$$
{{< /math >}}
As a consequence, we obtain that as $y_t \to \infty$,
the rate $k_t$ converges to $0$ faster than $y_t$ tends to $\infty$.
We obtain
{{< math >}}
$$
m_t \to m_{t-1} \text{ and } s_t \to s_{t-1} \text{ as } y_t \to \infty.
\tag{8}
$$
{{< /math >}}
In other words, with 1D Wolf, large and unexpected errors get discarded.
The larger the error, the less information it provides to the estimate $m_t$.

## Choice of weight function
The choice of the weight function $w_t$ in $(7)$ is crucial to the robustness of the WoLF method.
Following the WolF method, we consider the IMQ weight function
{{< math >}}
$$
w_t = \left(1 + \frac{(y_t - m_{t-1})^2}{c^2}\right)^{-1/2},
\tag{9}
$$
{{< /math >}}
where $\mu_{t-1}$ is the previous estimate of the signal $z_t$ and $c > 0$ is a fixed hyperparameter.


# Numerical experiments
Here, we provide an example of the WoLF method for the EWMA.
First, we define the IMQ weight function $(9)$ and the WoLF method $(7)$.
```python
import numpy as np
from numba import njit

@njit
def imq(err, c):
    return 1 / np.sqrt(1 + (err / c) ** 2

@njit
def wolf_step(y, m, s, q, r, c):
    # weight function and rate
    wt = imq(y - m, c) ** 2
    k = s / (s + q + r ** 2 / wt)
    # posterior mean and variance
    m = k * y + (1 - k) * m
    s = k * r ** 2
    return m, s

@njit
def wolf(y, m0, s0, q, r, c):
    m = m0
    s = s0
    m_hist = np.zeros_like(y)
    s_hist = np.zeros_like(y)
    for t, yt in enumerate(y):
        m, s = wolf_step(yt, m, s, q, r, c)
        m_hist[t] = m
        s_hist[t] = s
    return m_hist, s_hist
```


# Conclusion

[^1]: There are more assumptions in the SSM $(2)$ that we have not mentioned here,
but the reader can find them in Eubank's 2005 book on SSMs.

----
# Appendix
## Derivation of the 1d Kalman filter equations
Here, we derive the Kalman filter equations $(6)$ for the SSM $(2)$.
Suppose $p(z_0) = {\cal N}(z_0 \vert m_0, s_0)$.
Consider
$p(z_{t-1} \vert y_{1:t-1}) = {\cal N}(z_{t-1} \vert m_{t-1}, s_{t-1})$ and
$p(y_t \vert z_t) = {\cal N}(y_t \vert z_t, r_t)$.
Then, the prediction step is given by
{{< math >}}
$$
\begin{aligned}
p(z_t \vert y_{1:t-1}) &= \int p(z_t \vert z_{t-1})\,p(z_{t-1} \vert y_{1:t-1})\,dz_{t-1}\\
&= \int {\cal N}(z_t \vert z_{t-1}, q_t)\,{\cal N}(z_{t-1} \vert m_{t-1}, s_{t-1})\,dz_{t-1}\\
&= {\cal N}(z_t \vert m_{t-1}, s_{t-1} + q_t).
&= {\cal N}(z_t \vert m_{t-1}, s_{t|t-1}),
\end{aligned}
$$
{{< /math >}}
with $s_{t|t-1} = s_{t-1} + q_t$.

Next, we consider the update step.
We have
{{< math >}}
$$
\begin{aligned}
p(z_t \vert y_{1:t})
&\propto p(y_t \vert z_t)\,p(z_t \vert y_{1:t-1})\\
&= {\cal N}(y_t \vert z_t, r_t^2)\,{\cal N}(z_t \vert m_{t-1}, s_{t-1} + q_t^2).
\end{aligned}
$$
{{< /math >}}
To compute the posterior, consider the log-posterior function
{{< math >}}
$$
\begin{aligned}
\log p(z_t \vert y_{1:t})
&= -\frac{1}{s_{t|t-1}^2}(z_t - m_{t-1})^2 - \frac{1}{r_t^2}(y_t - z_t)^2 + \text{const}.\\
&= -\frac{1}{s_{t|t-1}}(z_t ^ 2 - 2z_t m_{t-1} + m_{t-1}^2) - \frac{1}{r_t^2}(y_t^2 - 2y_t z_t + z_t^2) + \text{const}.\\
&= -\frac{1}{s_{t|t-1}}(z_t^2 - 2z_tm_{t-1}) - \frac{1}{r_t^2}(z_t^2 - 2z_ty_t) + \text{const}.\\
&= -\left((s_{t|t-1}^{-2} + r_{t}^{-2})z_t^2 - 2z_t\left( \frac{m_{t-1}}{s_{t|t-1}^2} + \frac{y_t}{r_t^2} \right)\right) + \text{const}.\\
&= -\left(s_{t|t-1}^{-2} + r_t^{-2}\right)\left[z_t^2 - 2z_t\left(s_{t|t-1} + r_{t}^{-2}\right)^{-1}\left( \frac{m_{t-1}}{s_{t|t-1}^2} + \frac{y_t}{r_t^2} \right)\right] + \text{const.}\\
&= -\left(s_{t|t-1}^{-2} + r_t^{-2}\right)\left[z_t - \left(s_{t|t-1} + r_{t}^{-2}\right)^{-1}\left( \frac{m_{t-1}}{s_{t|t-1}^2} + \frac{y_t}{r_t^2} \right)\right]^2 + \text{const.}
\end{aligned}
$$
{{< /math >}}
where $\text{const.}$ denotes a constant that does not depend on $z_t$.
From the above, we see that the posterior $p(z_t \vert y_{1:t})$ is a Gaussian density with mean $m_t$ and variance $s_t^2$,
where
{{< math >}}
$$
\begin{aligned}
    m_t &= \left(s_{t|t-1} + r_{t}^{-2}\right)^{-1}\left( \frac{m_{t-1}}{s_{t|t-1}^2} + \frac{y_t}{r_t^2} \right),\\
    s_t^2 &= \left(s_{t|t-1}^{-2} + r_t^{-2}\right)^{-1}.
\end{aligned}
$$
{{< /math >}}
Next, we simplify the above expressions to obtain the Kalman filter equations $(6)$.
For the posterior mean $m_t$, we have
{{< math >}}
$$
\begin{aligned}
    m_t
    &= \left(s_{t|t-1}^2 + r_{t}^{-2}\right)^{-1}\left( \frac{m_{t-1}}{s_{t|t-1}^2} + \frac{y_t}{r_t^2} \right),\\
    &= \frac{s_{t|t-1}^2r_t^2}{s_{t|t-1} + r_t^2}\left( \frac{r_t^2m_{t-1} + s_{t|t-1}^2y_t}{s_{t|t-1}^2r_t^2} \right),\\
    &= \frac{r_t^2}{s_{t|t-1}^2 + r_t^2}m_{t-1} + \frac{s_{t|t-1}^2}{s_{t|t-1}^2 + r_t^2}y_t,\\
    &= \left(1 - \frac{s_{t|t-1}^2}{s_{t|t-1}^2 +r_t^2}\right)m_{t-1} + \frac{s_{t|t-1}^2}{s_{t|t-1}^2 + r_t^2}y_t,\\
    &= (1 - k_t)m_{t-1} + k_ty_t.
\end{aligned}
$$
{{< /math >}}
with $k_t = s_{t|t-1}^2 / (s_{t|t-1}^2 + r_t^2)$.
Next, we compute the posterior variance $s_t^2$.
We have
{{< math >}}
$$
\begin{aligned}
    s_t^2
    &= \frac{1}{s_{t|t-1}^{-2} + r_t^{-2}}\\
    &= \frac{s_{t|t-1}^2r_t^2}{s_{t|t-1}^2 + r_t^2}\\
    &= \left(\frac{s_{t|t-1}^2}{s_{t|t-1}^2 r_t^2}\right)r_t^2\\
    &= k_tr_t^2.
\end{aligned}
$$
{{< /math >}}

