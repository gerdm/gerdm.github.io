---
title: "An outlier-robust EWMA"
date: 2024-07-13
description: "WoLF for robust estimation of an exponentially weighted moving average."
katex: true
draft: true
tags: [kalman-filter, wolf, ewma]
tldr: "An outlier robust exponentially weighted moving average via the WoLF method."
---

Our weighted-observation likelihood filter (WolF) got accepted at ICML 2024.
Our method, which is provably-robust and easy-to-implement, *robustifies* the Kalman filter (KF)
by replacing the classical Gaussian likelihood assumption with a loss function.
Despite its simplicity, the terminology behind WoLF (similar to the terminology of the KF), might not be familiar to everyone
and the method might seem a bit abstract for newcomers.

Thus, to show the practical utility of our method,
we will show how to create a robust variant of the KF that is familiar to most: the exponentially weighted moving average (EWMA).

This post is organlised as follows:
first, we recap the EWMA.
Then, we introduce a one-dimensional state-space model (SSM) and show that the EWMA is a special case of the KF in this SSM setting.
Next, we derive the WoLF method for an EWMA.
We conclude this post by showing a numerical experiment that illustrates the robustness of the WoLF method in one-dimensional financial data.

# The exponentially weighted moving average (EWMA)
Given a sequence of observations (or measurements) $y_{1:t} = (y_1, \ldots, y_t)$,
the EWMA of the observations at time $t$ is given by
{{< math >}}
$$
m_t = \beta\,y_t + (1-\beta)\,m_{t-1},
\tag{1}
$$
{{< /math >}}
where $\beta \in (0,1]$ is the smoothing factor.
Higher levels of $\beta$ give more weight to recent observations.

# The Kalman filter in one dimension
Consider the following SSM:

{{< math >}}
$$
\begin{aligned}
z_t &= z_{t-1} + w_t,\\
y_t &= z_t + e_t,
\end{aligned}
\tag{2}
$$
{{< /math >}}
where $z_t$ is the state, $w_t$ is the process noise, and $v_t$ is the measurement noise.
We assume that ${\rm var}(w_t) = q_t$ and ${\rm var}(e_t) = r_t$.[^1]
Put simply, the SSM model $(2)$ assumes that the observations $y_t$ are generated by a signal $z_t$ plus noise $e_t$.
Our goal with the Kalman filter is to estimate the signal $z_t$ given the observations $y_{1:t}$.

One approach to estimate $z_t$ is to assume prior knowledge about the
initial signal $z_0$, the state-transition dynamics $z_t \vert z_{t-1}$, and the observation model $y_t \vert z_t$.
We write 
{{< math >}}
$$
\begin{aligned}
p(z_0) &= {\cal N}(z_0 \vert m_0, s_0),\\
p(z_t \vert z_{t-1}) &= {\cal N}(z_t \vert z_{t-1}, q_t),\\
p(y_t \vert z_t) &= {\cal N}(y_t \vert z_t, r_t).
\end{aligned}
\tag{3}
{{< /math >}}
$$
where ${\cal N}(\cdot \vert \mu, \sigma^2)$ denotes the Gaussian distribution with mean $\mu$ and variance $\sigma^2$.
Our estimate of the signal $z_t$ given the observations $y_{1:t}$ is given by
{{< math >}}
$$
m_t = \mathbb{E}[z_t \vert y_{1:t}] = \int z_t\,p(z_t \vert y_{1:t}) dz_t,
\tag{4}
{{< /math >}}
$$
where $\mathbb{E}[\cdot]$ denotes the expectation operator.

The main quantity of interest in the Kalman filter in this perspective is the posterior distribution $p(z_t \vert y_{1:t})$,
which we can compute recursively using Bayes' rule.
Assuming the initial density $p(z_0) = {\cal N}(z_0 \vert m_0, s_0)$,
the posterior density $p(z_t \vert y_{1:t})$ is given by
{{< math >}}
$$
p(z_t \vert y_{1:t}) \propto p(y_t \vert z_t) p(z_t \vert y_{1:t-1}) = {\cal N}(z_t \vert m_t, s_t),
\tag{5}
$$
{{< /math >}}
with
{{< math >}}
$$
\begin{aligned}
k_t &= \frac{s_{t-1} + q_t}{s_{t-1} + q_t + r_t},\\
s_t &= k_t\,r_t,\\
m_t &= k_t\,y_t + (1-k_t)\,m_{t-1}.
\end{aligned}
\tag{6}
$$
{{< /math >}}
We defer the derivation of $(6)$ to the appendix.
From $(6)$, we see that the KF with SSM $(2)$ is equivalent to the EWMA with $\beta$ replaced by $k_t$,
i.e., **the KF is an EWMA with a time-varying smoothing factor**.
The KF formulation of the EWMA from a KF point of view
(i) provides a principled way to update the smoothing factor $k_t$,
(ii) and provides a way to estimate the uncertainty of the signal $z_t$, and
(iii) it allows us to derive the WoLF variant for the EWMA.
We do this in the next section.


# The WoLF method for the EWMA
To create a 1D version of WoLF, we
consider the SSM $(2)$ with $q_t = q$ and $r_t = r / w_t$. Here $q \geq 0$ and $r > 0$ are fixed hyperparameters.

# Numerical experiments

# Conclusion

[^1]: There are more assumptions in the SSM $(2)$ that we have not mentioned here,
but the reader can find them in Eubank's 2005 book on SSMs.